//! Markdown generation for benchmark results
//!
//! This module provides functionality for generating markdown reports
//! from benchmark results.

use super::result::BenchmarkResult;
use super::io::{summary_file, read_raw_results};
use std::fs;
use std::io::{self, Write};
use std::path::Path;

/// Generate a markdown summary from benchmark results
pub fn generate_summary(results: &[BenchmarkResult]) -> String {
    let mut md = String::new();

    md.push_str("# Benchmark Summary\n\n");
    md.push_str("This file contains benchmark results for LLM Config Manager.\n\n");

    if results.is_empty() {
        md.push_str("_No benchmark results available._\n");
        return md;
    }

    // Group results by target_id
    let mut grouped: std::collections::HashMap<String, Vec<&BenchmarkResult>> =
        std::collections::HashMap::new();
    for result in results {
        grouped
            .entry(result.target_id.clone())
            .or_default()
            .push(result);
    }

    // Add timestamp
    if let Some(latest) = results.iter().max_by_key(|r| r.timestamp) {
        md.push_str(&format!(
            "**Last updated:** {}\n\n",
            latest.timestamp.format("%Y-%m-%d %H:%M:%S UTC")
        ));
    }

    md.push_str("## Results by Target\n\n");

    // Create results table
    md.push_str("| Target | Duration (ms) | Throughput (ops/s) | Timestamp |\n");
    md.push_str("|--------|---------------|-------------------|----------|\n");

    let mut sorted_targets: Vec<_> = grouped.keys().collect();
    sorted_targets.sort();

    for target_id in &sorted_targets {
        if let Some(results) = grouped.get(*target_id) {
            // Get the latest result for this target
            if let Some(latest) = results.iter().max_by_key(|r| r.timestamp) {
                let duration_ms = latest
                    .metrics
                    .get("duration_ms")
                    .and_then(|v| v.as_f64())
                    .map(|v| format!("{:.3}", v))
                    .unwrap_or_else(|| "-".to_string());

                let throughput = latest
                    .throughput_ops_per_sec()
                    .map(|v| format!("{:.2}", v))
                    .unwrap_or_else(|| "-".to_string());

                let timestamp = latest.timestamp.format("%Y-%m-%d %H:%M:%S");

                md.push_str(&format!(
                    "| {} | {} | {} | {} |\n",
                    target_id, duration_ms, throughput, timestamp
                ));
            }
        }
    }

    md.push_str("\n## Detailed Results\n\n");

    for target_id in &sorted_targets {
        if let Some(results) = grouped.get(*target_id) {
            if let Some(latest) = results.iter().max_by_key(|r| r.timestamp) {
                md.push_str(&format!("### {}\n\n", target_id));
                md.push_str("```json\n");
                if let Ok(json) = serde_json::to_string_pretty(&latest.metrics) {
                    md.push_str(&json);
                }
                md.push_str("\n```\n\n");
            }
        }
    }

    md.push_str("---\n\n");
    md.push_str("*Generated by LLM Config Manager Benchmark Suite*\n");

    md
}

/// Write the summary markdown file
pub fn write_summary(base_path: &Path, results: &[BenchmarkResult]) -> io::Result<()> {
    let summary_path = summary_file(base_path);

    // Ensure parent directory exists
    if let Some(parent) = summary_path.parent() {
        fs::create_dir_all(parent)?;
    }

    let markdown = generate_summary(results);
    let mut file = fs::File::create(&summary_path)?;
    file.write_all(markdown.as_bytes())?;

    Ok(())
}

/// Update the summary file with new results
pub fn update_summary(base_path: &Path) -> io::Result<()> {
    let results = read_raw_results(base_path)?;
    write_summary(base_path, &results)
}

/// Generate a comparison report between two sets of results
pub fn generate_comparison(
    baseline: &[BenchmarkResult],
    current: &[BenchmarkResult],
) -> String {
    let mut md = String::new();

    md.push_str("# Benchmark Comparison\n\n");

    if baseline.is_empty() || current.is_empty() {
        md.push_str("_Insufficient data for comparison._\n");
        return md;
    }

    md.push_str("| Target | Baseline (ms) | Current (ms) | Change |\n");
    md.push_str("|--------|---------------|--------------|--------|\n");

    // Create maps for quick lookup
    let baseline_map: std::collections::HashMap<_, _> = baseline
        .iter()
        .map(|r| (r.target_id.clone(), r))
        .collect();

    let current_map: std::collections::HashMap<_, _> = current
        .iter()
        .map(|r| (r.target_id.clone(), r))
        .collect();

    // Get all unique targets
    let mut all_targets: std::collections::HashSet<_> = baseline_map.keys().collect();
    all_targets.extend(current_map.keys());

    let mut sorted_targets: Vec<_> = all_targets.into_iter().collect();
    sorted_targets.sort();

    for target_id in sorted_targets {
        let baseline_ms = baseline_map
            .get(target_id)
            .and_then(|r| r.metrics.get("duration_ms"))
            .and_then(|v| v.as_f64());

        let current_ms = current_map
            .get(target_id)
            .and_then(|r| r.metrics.get("duration_ms"))
            .and_then(|v| v.as_f64());

        let baseline_str = baseline_ms
            .map(|v| format!("{:.3}", v))
            .unwrap_or_else(|| "-".to_string());

        let current_str = current_ms
            .map(|v| format!("{:.3}", v))
            .unwrap_or_else(|| "-".to_string());

        let change = match (baseline_ms, current_ms) {
            (Some(b), Some(c)) => {
                let pct = ((c - b) / b) * 100.0;
                if pct > 5.0 {
                    format!("+{:.1}% (slower)", pct)
                } else if pct < -5.0 {
                    format!("{:.1}% (faster)", pct)
                } else {
                    "~same".to_string()
                }
            }
            _ => "-".to_string(),
        };

        md.push_str(&format!(
            "| {} | {} | {} | {} |\n",
            target_id, baseline_str, current_str, change
        ));
    }

    md
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[test]
    fn test_generate_summary_empty() {
        let summary = generate_summary(&[]);
        assert!(summary.contains("No benchmark results available"));
    }

    #[test]
    fn test_generate_summary_with_results() {
        let results = vec![
            BenchmarkResult::timing("config_get", 1_000_000),
            BenchmarkResult::throughput("config_set", 1_000_000_000, 1000),
        ];

        let summary = generate_summary(&results);
        assert!(summary.contains("config_get"));
        assert!(summary.contains("config_set"));
        assert!(summary.contains("| Target |"));
    }

    #[test]
    fn test_write_summary() {
        let temp_dir = TempDir::new().unwrap();
        let results = vec![BenchmarkResult::timing("test_target", 1000)];

        write_summary(temp_dir.path(), &results).unwrap();

        let summary_path = summary_file(temp_dir.path());
        assert!(summary_path.exists());

        let content = fs::read_to_string(summary_path).unwrap();
        assert!(content.contains("test_target"));
    }

    #[test]
    fn test_generate_comparison() {
        let baseline = vec![BenchmarkResult::timing("test", 1_000_000)];
        let current = vec![BenchmarkResult::timing("test", 1_100_000)];

        let comparison = generate_comparison(&baseline, &current);
        assert!(comparison.contains("test"));
        assert!(comparison.contains("Baseline"));
        assert!(comparison.contains("Current"));
    }
}
